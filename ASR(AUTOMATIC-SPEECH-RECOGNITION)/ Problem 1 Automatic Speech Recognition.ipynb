{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Forward algorithm, 1.2 Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "states = [\"red\", \"green\", \"blue\"] # given states\n",
    "initial_prob = [1.0, 0, 0 ] # given probability\n",
    "\n",
    "# given A matrix\n",
    "A = [[0.6,0.4,0],\n",
    "    [0,0.3,0.7],\n",
    "    [0,0,1]]\n",
    "# combined b0, b1, b2\n",
    "B = [[0.8, 0.65, 0.20],\n",
    "    [0.15, 0.10, 0.30],\n",
    "    [0.05, 0.25, 0.50]]\n",
    "observation_seq = [1,1,3,2,1,3] #red red blue green red blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emissionProb(state_id, observation):\n",
    "    result = B[state_id][observation-1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward table:\n",
      " [[8.00000000e-01 3.84000000e-01 4.60800000e-02 1.79712000e-02\n",
      "  8.62617600e-03 1.03514112e-03]\n",
      " [0.00000000e+00 4.80000000e-02 5.04000000e-02 3.35520000e-03\n",
      "  1.22925600e-03 1.14577416e-03]\n",
      " [0.00000000e+00 0.00000000e+00 1.68000000e-02 1.30200000e-02\n",
      "  7.68432000e-04 8.14455600e-04]]\n",
      "\n",
      "Backtrace Table:\n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 2. 1.]]\n",
      "\n",
      "Best Hidden Sequence derderderderderneerg\n",
      "\n",
      "Probability of Observating the sequence “red red blue green red blue”: 0.00299537088\n"
     ]
    }
   ],
   "source": [
    "def HMM(states, initial_prob, B, observation_seq, A):\n",
    "    table = np.zeros((len(states),len(observation_seq)))\n",
    "    back_trace_table = np.zeros((len(states),len(observation_seq)))\n",
    "    \n",
    "    #Initialization\n",
    "    for i in range(len(states)):\n",
    "        table[i][0] = initial_prob[i]*emissionProb(i,observation_seq[0])\n",
    "    \n",
    "    #Forward Algorithm - Recursion\n",
    "    for observation_id in range(1,len(observation_seq),1):\n",
    "        for state_id in range(len(states)):\n",
    "            paths = []\n",
    "            for path_id in range(len(states)):\n",
    "                element1 = table[path_id][observation_id-1]\n",
    "                element2 = A[path_id][state_id]\n",
    "                element3 = emissionProb(state_id, observation_seq[observation_id])\n",
    "                path_element = element1*element2*element3\n",
    "                paths.append(path_element)\n",
    "            table[state_id][observation_id] = sum(paths)\n",
    "            back_trace_table[state_id][observation_id] = np.argmax(paths)\n",
    "\n",
    "    #Viterbi Decoding\n",
    "    max_state_id = int(np.argmax(table[:,-1]))\n",
    "    state_pointer = int(back_trace_table[max_state_id][-1])\n",
    "    inverse_best_seq = states[max_state_id]\n",
    "    for observation_id in range(len(observation_seq)-2,-1,-1):\n",
    "        inverse_best_seq += states[state_pointer]\n",
    "        state_pointer = int(back_trace_table[state_pointer][observation_id])\n",
    "    best_sequence = inverse_best_seq[::-1]\n",
    "    #print(\"inverse best seq\", inverse_best_seq)\n",
    "    prob = [table[i][-1] for i in range(table.shape[0])]\n",
    "    probability_of_observation_seq = sum(prob)\n",
    "    \n",
    "    print(\"Forward table:\\n\",table)\n",
    "    print(\"\")\n",
    "    print(\"Backtrace Table:\\n\",back_trace_table)\n",
    "    print(\"\")\n",
    "    print(\"Best Hidden Sequence\",best_sequence)\n",
    "    print(\"\")\n",
    "    print(\"Probability of Observating the sequence “red red blue green red blue”:\",probability_of_observation_seq)\n",
    "    \n",
    "    return probability_of_observation_seq\n",
    "probability_of_observation_seq = HMM(states, initial_prob, B, observation_seq, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 N-Gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model to compute the probability of the sencence: FROM DENVER TO SEATTLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "from nltk.tokenize import word_tokenize as wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dict = {}\n",
    "bigram_dict = {}\n",
    "trigram_dict = {}\n",
    "SENTENCE_START = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "unigram_corpus_length = 0\n",
    "bigram_corpus_length = 0\n",
    "trigram_corpus_length = 0\n",
    "unigram_unique_words = 0\n",
    "bigram_unique_words = 0\n",
    "trigram_unique_words = 0\n",
    "bigram_unique_set = set()\n",
    "trigram_unique_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcgram(s):\n",
    "    return len(s) - 2\n",
    "\n",
    "def calculate_unigram(s):\n",
    "    res = 0\n",
    "    for sentence in s:\n",
    "        res += len(sentence) - 2\n",
    "    return res\n",
    "\n",
    "def calculate_bigram(s):\n",
    "    res = 0\n",
    "    for sentence in s:\n",
    "        res += len(sentence) - 1\n",
    "    return res\n",
    "\n",
    "def calculate_trigram(s):\n",
    "    res = 0\n",
    "    for sentence in s:\n",
    "        res += len(sentence) - 2\n",
    "    return res\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    new_words = [SENTENCE_START]\n",
    "    for word in wt(s):\n",
    "        new_words.append(word)\n",
    "    new_words.append(SENTENCE_END)\n",
    "    return new_words\n",
    "\n",
    "def preprocess_sentences(s):\n",
    "    sentences = st(s)\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append(preprocess_sentence(sentence[:-1])[:])\n",
    "    return new_sentences\n",
    "\n",
    "def preprocess(s):\n",
    "    unigram(s)\n",
    "    bigram(s)\n",
    "    trigram(s)\n",
    "    \n",
    "def unigram(s):\n",
    "    '''Unigram model'''\n",
    "    global unigram_corpus_length\n",
    "    for sentence in s:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                unigram_dict[word] += 1\n",
    "            except:\n",
    "                unigram_dict[word] = 1\n",
    "            if(word != SENTENCE_START and word != SENTENCE_END):\n",
    "                unigram_corpus_length += 1\n",
    "    unigram_unique_words = len(unigram_dict) - 2  \n",
    "\n",
    "def bigram(s):\n",
    "    '''Bigrm model'''\n",
    "    global bigram_corpus_length\n",
    "    for sentence in s:\n",
    "        previous_word = None\n",
    "        for word in sentence:\n",
    "            if(previous_word != None):\n",
    "                try:\n",
    "                    bigram_dict[(previous_word,word)] += 1\n",
    "                except:\n",
    "                    bigram_dict[(previous_word,word)] = 1\n",
    "                if previous_word != SENTENCE_START and word != SENTENCE_END:\n",
    "                    bigram_unique_set.add((previous_word, word))\n",
    "            previous_word = word\n",
    "    bigram_unique_words = len(bigram_dict)\n",
    "                    \n",
    "def trigram(s):\n",
    "    '''Trigram model'''\n",
    "    global trigram_corpus_length\n",
    "    for sentence in s:\n",
    "        first_word = None\n",
    "        second_word = None\n",
    "        for word in sentence:\n",
    "            if(first_word != None and second_word != None):\n",
    "                try:\n",
    "                    trigram_dict[(first_word,second_word,word)] += 1\n",
    "                except:\n",
    "                    trigram_dict[(first_word,second_word,word)] = 1\n",
    "                if first_word != SENTENCE_START and word != SENTENCE_END:\n",
    "                    trigram_unique_set.add((first_word,second_word,word))\n",
    "            first_word = second_word\n",
    "            second_word = word\n",
    "    trigram_unique_words = len(trigram_dict)\n",
    "                    \n",
    "\n",
    "def probability_unigram(word,smooth):\n",
    "    try:\n",
    "        result_numerator = unigram_dict[word]\n",
    "    except:\n",
    "        result_numerator = 0\n",
    "    try:\n",
    "        result_denumerator = unigram_unique_words\n",
    "    except:\n",
    "        result_denumerator = 0\n",
    "    if(smooth):\n",
    "        result_numerator += 1\n",
    "        result_denumerator += 1\n",
    "    return float(result_numerator) / float(result_denumerator)\n",
    "\n",
    "def probability_bigram(previous_word,word,smooth):\n",
    "    try:\n",
    "        result_numerator = bigram_dict[(previous_word,word)]\n",
    "    except:\n",
    "        result_numerator = 0\n",
    "    try:\n",
    "        result_denumerator = unigram_dict[previous_word]\n",
    "    except:\n",
    "        result_denumerator = 0\n",
    "    if(smooth):\n",
    "        result_numerator += 1\n",
    "        result_denumerator += 1\n",
    "    if(result_numerator == 0 or result_denumerator ==0):\n",
    "        return 0.0\n",
    "    return float(result_numerator) / float(result_denumerator)\n",
    "\n",
    "def probability_trigram(first_word,second_word,word,smooth):\n",
    "    try:\n",
    "        result_numerator = trigram_dict[(first_word,second_word,word)]\n",
    "    except:\n",
    "        result_numerator = 0\n",
    "    try:\n",
    "        result_denumerator = bigram_dict[(first_word,second_word)]\n",
    "    except:\n",
    "        result_denumerator = 0\n",
    "    if(smooth):\n",
    "        result_numerator += 1\n",
    "        result_denumerator += 1\n",
    "    if(result_numerator == 0 or result_denumerator ==0):\n",
    "        return 0.0\n",
    "    return float(result_numerator) / float(result_denumerator)\n",
    "\n",
    "def sentence_probability(gram_type,sentence,smooth):\n",
    "    '''Function to calculate the sentence probability'''\n",
    "    result = 0.0\n",
    "    if(gram_type == \"Unigram\"):\n",
    "        for word in sentence :\n",
    "            if(word != SENTENCE_START and word != SENTENCE_END):\n",
    "                if(result == 0.0):\n",
    "                    result = probability_unigram(word,smooth)\n",
    "                else:\n",
    "                    result *= probability_unigram(word,smooth)\n",
    "\n",
    "    elif(gram_type == \"Bigram\"):\n",
    "        previous_word = None\n",
    "        for word in sentence:\n",
    "            if(previous_word != None  and word != SENTENCE_END):\n",
    "                if(result == 0.0):\n",
    "                    result = probability_bigram(previous_word,word,smooth)\n",
    "                else:\n",
    "                    result *= probability_bigram(previous_word,word,smooth)\n",
    "            previous_word = word\n",
    "\n",
    "    elif(gram_type ==\"Trigram\"):\n",
    "        first_word = None\n",
    "        second_word = None\n",
    "        for word in sentence:\n",
    "            if(first_word != None and word != SENTENCE_END):\n",
    "                if(result == 0.0):\n",
    "                    result = probability_trigram(first_word,second_word,word,smooth)\n",
    "                else:\n",
    "                    result *= probability_trigram(first_word,second_word,word,smooth)\n",
    "            previous_word = word\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence probability is: 0.007407407407407408\n"
     ]
    }
   ],
   "source": [
    "train_sentences = \"I WANT TO GO FROM DENVER TO BOSTON TOMORROW MORNING. I WOULD LIKE TO GO TO SEATTLE THIS AFTERNOON. DEPARTING SEATTLE. IN THE LATE MORNING. LATE MORNING GOING TO DENVER FROM BOSTON. FROM DENVER LEAVING IN THE MORNING ARRING IN BOSTON.\"\n",
    "sentences = preprocess_sentences(train_sentences)\n",
    "preprocess(sentences)\n",
    "input_sentences = preprocess_sentence(\"FROM DENVER TO SEATTLE \")\n",
    "print('The sentence probability is:',sentence_probability(\"Bigram\",input_sentences,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
